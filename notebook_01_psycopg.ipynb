{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ccf013-b6b8-4027-a0fe-d90c619c5782",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Psycopg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fab212-642d-46a9-9b1b-6502c4183694",
   "metadata": {},
   "source": [
    "Let's import `psycopg` and make a simple query. This requires:\n",
    "- preparing connection string (see [Postgres docs](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING))\n",
    "- making a connection\n",
    "- creating a cursor\n",
    "- running a query on the cursor\n",
    "- parsing results\n",
    "\n",
    "See the code below and comments. As our query, we will read:\n",
    "- total number of trains stopping in Amsterdam\n",
    "- all trains that were late in Amsterdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea898094-1ff8-45be-b564-46ad3b9d07d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host=localhost user=postgres password=postgres dbname=postgres\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "\n",
    "# build the connection string\n",
    "host = \"localhost\"\n",
    "username = \"postgres\"\n",
    "password = \"postgres\"  # NEVER do this in real code, read from secure secrets file\n",
    "db_name = \"postgres\"\n",
    "\n",
    "db_conn_string = f\"host={host} user={username} password={password} dbname={db_name}\"\n",
    "print(db_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0fb2dd-e745-4cb3-9206-9d24afec820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trains: (404915,)\n",
      "Late trains:\n",
      "(15063522, datetime.date(2024, 12, 27), 'Intercity', 'NS', 1573, False, False, 1, 135937936, 'ASD', 'Amsterdam Centraal', None, None, None, datetime.datetime(2024, 12, 27, 20, 0), 1, False, True, '8a', '8')\n",
      "(15063589, datetime.date(2024, 12, 27), 'Sprinter', 'NS', 8375, False, False, 2, 135938564, 'ASD', 'Amsterdam Centraal', None, None, None, datetime.datetime(2024, 12, 27, 20, 13), 2, False, False, '10a', '10a')\n",
      "(15063590, datetime.date(2024, 12, 27), 'Intercity', 'NS', 3072, False, False, 0, 135938576, 'ASD', 'Amsterdam Centraal', datetime.datetime(2024, 12, 27, 21, 35), 7, False, datetime.datetime(2024, 12, 27, 21, 39), 4, False, False, '7a', '7a')\n"
     ]
    }
   ],
   "source": [
    "query_total_trains = \"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM services\n",
    "WHERE \"Stop:Station code\" = 'ASD'\n",
    "\"\"\"\n",
    "\n",
    "query_late_departure_trains = \"\"\"\n",
    "SELECT *\n",
    "FROM services\n",
    "WHERE \"Stop:Station code\" = 'ASD' AND \"Stop:Departure delay\" > 0\n",
    "\"\"\"\n",
    "\n",
    "# connect to the database (context manager will automatically close connection)\n",
    "with psycopg.connect(db_conn_string) as conn:\n",
    "    # create a cursor to execute commands\n",
    "    with conn.cursor() as cur:\n",
    "        # send query and fetch result\n",
    "        cur.execute(query_total_trains)\n",
    "        total_trains = cur.fetchone()\n",
    "\n",
    "        # send query and fetch results\n",
    "        cur.execute(query_late_departure_trains)\n",
    "        late_trains = cur.fetchall()\n",
    "\n",
    "print(\"Total trains:\", total_trains)\n",
    "\n",
    "print(\"Late trains:\")\n",
    "for record in late_trains[:3]:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500da471-53bf-43a2-998e-07d8f2a2d1af",
   "metadata": {},
   "source": [
    "**Connection** manages database session and network connection. It should always be used as a context manager, to close it after usage, since keeping \"dangling\" open connections wastes system resources. Opening and closing connections has some cost, so for batches of tasks it's good to keep the connection open.\n",
    "\n",
    "**Connection string** defines the Postgres connection. We can either build a string, or pass keyword arguments to `.connect()`.\n",
    "\n",
    "**Cursor** executes commands to the database. It will always use a transaction in psycopg. If you don't explicitly commit by calling `conn.commit()` inside the cursor block, it will roll back all changes. However, here we only read data, so this doesn't matter.\n",
    "\n",
    "**Results** are by default returned as a tuple for `.fetchone()` (even a single element), or list of tuples for `.fetchall()`. Types are automatically parsed, e.g. dates are `datetime.date` objects, with exact rules [in the documentation](https://www.psycopg.org/psycopg3/docs/basic/adapt.html).\n",
    "\n",
    "Tuples are not always convenient, and we have two main options:\n",
    "- use [row factory](https://www.psycopg.org/psycopg3/docs/advanced/rows.html#row-factories) to return dictionaries, named tuples, or a custom dataclass (you need to write it first)\n",
    "- use an ORM like SQLAlchemy or peewee\n",
    "\n",
    "Which one makes the most sense depends on a use case. If you use a custom dataclass, you are really close to using an ORM, so the line is often blurry.\n",
    "\n",
    "**Efficiency** typically has tradeoffs for large data. Processing data in sizes is typically necessary, as we can't easily load all data into memory like here. Smaller batches are sometimes more efficient within the database, particularly when we have proper indexes. For example, you may want to query for a single month at a time. However, too small batches will waste resources on network requests.\n",
    "\n",
    "**Column names** are often useful for automation, and we can get them without wasting a lot of memory for dictionaries. In fact, creating Pandas DataFrames from default tuples is actually faster and easier. We can get the list of column names from the cursor `description` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1eedc1-6868-4342-81e5-2dbb39b34b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Service:RDT-ID', 'Service:Date', 'Service:Type', 'Service:Company', 'Service:Train number', 'Service:Completely cancelled', 'Service:Partly cancelled', 'Service:Maximum delay', 'Stop:RDT-ID', 'Stop:Station code', 'Stop:Station name', 'Stop:Arrival time', 'Stop:Arrival delay', 'Stop:Arrival cancelled', 'Stop:Departure time', 'Stop:Departure delay', 'Stop:Departure cancelled', 'Stop:Platform change', 'Stop:Planned platform', 'Stop:Actual platform']\n"
     ]
    }
   ],
   "source": [
    "with psycopg.connect(db_conn_string) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # LIMIT 0 for efficiency - we don't want rows, just column names\n",
    "        cur.execute(\"SELECT * FROM services LIMIT 0\")\n",
    "        colnames = [desc[0] for desc in cur.description]\n",
    "        print(colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab781a-809f-4c33-8d9d-2b7aa0a4395f",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "\n",
    "1. Write a query to get all trains that both arrived and departured late.\n",
    "\n",
    "2. Use a row factory to return dictionaries. \n",
    "```python\n",
    "    (from psycopg.rows import dict_row)\n",
    "```\n",
    "\n",
    "3. Save the result to file `data/late_train.jsonl` in JSON Lines format. `json` module will be useful.\n",
    "\n",
    "Since JSON does not have a default date or datetime format, we need to provide an explicit encoder for that. Pass this class to `json.dump()` as `cls` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5c9e66-846d-4a34-b2ab-25c388232448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "class DateEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (datetime.date, datetime.datetime)):\n",
    "            return obj.isoformat()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg.rows import dict_row\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "QUERY = 'SELECT * FROM services s WHERE s.\"Stop:Arrival delay\" > 0 AND s.\"Stop:Departure delay\" > 0'\n",
    "\n",
    "RESULT_FILE = \"labs/mlops_lab2/data/late_train.jsonl\"\n",
    "os.makedirs(os.path.dirname(RESULT_FILE), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac953927",
   "metadata": {},
   "source": [
    "### Naive Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea22cee1-ba3b-409c-8cd1-41e701441a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 1000000 rows in 124.40\n",
      "Written 2000000 rows in 247.80\n",
      "Written 3000000 rows in 373.31\n",
      "Written 4000000 rows in 496.98\n",
      "Written 5000000 rows in 604.99\n",
      "Duration: 616.44, count: 5098078\n"
     ]
    }
   ],
   "source": [
    "naive_start_time = time.perf_counter()\n",
    "count = 0\n",
    "\n",
    "with psycopg.connect(db_conn_string, row_factory=dict_row) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(QUERY)\n",
    "\n",
    "        with open(RESULT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            for row in cur:\n",
    "                json.dump(row, fp=f, cls=DateEncoder)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                if count % 1000000 == 0:\n",
    "                    print(\n",
    "                        f\"Written {count} rows in {time.perf_counter() - naive_start_time:.2f}\"\n",
    "                    )\n",
    "\n",
    "naive_end_time = time.perf_counter()\n",
    "duration = naive_end_time - naive_start_time\n",
    "print(f\"Duration: {duration:.2f}, count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d18045",
   "metadata": {},
   "source": [
    "### Pandas solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3b89a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22015/17835107.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  for chunk_df in pandas.read_sql_query(QUERY, con=conn, chunksize=CHUNKS_SIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 250000 rows\n",
      "Loaded 500000 rows\n",
      "Loaded 750000 rows\n",
      "Loaded 1000000 rows\n",
      "Loaded 1250000 rows\n",
      "Loaded 1500000 rows\n",
      "Loaded 1750000 rows\n",
      "Loaded 2000000 rows\n",
      "Loaded 2250000 rows\n",
      "Loaded 2500000 rows\n",
      "Loaded 2750000 rows\n",
      "Loaded 3000000 rows\n",
      "Loaded 3250000 rows\n",
      "Loaded 3500000 rows\n",
      "Loaded 3750000 rows\n",
      "Loaded 4000000 rows\n",
      "Loaded 4250000 rows\n",
      "Loaded 4500000 rows\n",
      "Loaded 4750000 rows\n",
      "Loaded 5000000 rows\n",
      "Loaded 5098078 rows\n",
      "Duration: 402.06, count: 5098078\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "CHUNKS_SIZE = 250_000\n",
    "pandas_start_time = time.perf_counter()\n",
    "count = 0\n",
    "\n",
    "with psycopg.connect(db_conn_string) as conn:\n",
    "    for chunk_df in pandas.read_sql_query(QUERY, con=conn, chunksize=CHUNKS_SIZE):\n",
    "        chunk_df.to_json(RESULT_FILE, orient=\"records\", lines=True, mode=\"a\")\n",
    "        count += chunk_df.shape[0]\n",
    "\n",
    "        print(f\"Loaded {count} rows\")\n",
    "\n",
    "\n",
    "pandas_end_time = time.perf_counter()\n",
    "pandas_duration = pandas_end_time - pandas_start_time\n",
    "print(f\"Duration: {pandas_duration:.2f}, count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccddb0",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Pandas solution is faster :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
